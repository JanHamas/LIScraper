Indeed AI Job Lead Scraper
Your AI-Powered Gateway to Perfectly Matched Job Leads

Tired of manually sifting through thousands of job listings? This isn't just a scraper; it's an intelligent, automated lead generation engine. It leverages the power of AI, advanced anti-detection techniques, and seamless Google Sheets integration to deliver a curated list of high-quality job opportunities directly to you, saving you hundreds of hours and connecting you with the best roles faster than ever before.

âœ¨ Key Features: The Power Under the Hood

This scraper is built with cutting-edge technology to ensure performance, reliability, and unparalleled results.

ğŸš€ Dynamic Control via Google Sheets:

No Code Required! Manage the entire scraping process from a simple Google Sheet. Update job search URLs, target companies, ignore lists, and even the AI prompt on the fly.

ğŸ§  AI-Powered Job Matching:

Stop wasting time on irrelevant jobs. Our system uses a powerful AI model (Google Gemini) to analyze your resume and custom instructions, then scores each job listing for relevance. You only see the jobs that perfectly match your criteria.

ğŸ›¡ï¸ Advanced Anti-Bot Evasion:

Human-Like Behavior: Simulates human scrolling, mouse movements, and random delays to remain undetected.

Unique Browser Fingerprints: Each scraping session uses a unique, realistic browser fingerprint to avoid common detection patterns.

Rotating Proxies & Accounts: Automatically rotates through a list of proxies and Indeed accounts (cookies) for maximum anonymity and resilience.

ğŸ¤– Automated CAPTCHA Solving:

Cloudflare and other bot protections are no match. The scraper automatically detects and solves CAPTCHAs (like Turnstile) using the 2Captcha service, ensuring uninterrupted operation.

ğŸ“ˆ Intelligent Job Classification:

Jobs are automatically sorted into three powerful categories:

Easy Applies: Jobs with simple, direct application processes.

Company Site Applies: Jobs that require applying on the company's own website.

Confirmation Companies: High-priority jobs from your personal "must-apply" list of companies.

ğŸ“Š Robust Data Handling & Automated Reporting:

Real-time CSV Output: Scraped data is saved locally into organized CSV files.

Automatic Google Sheets Upload: Once the scraping is complete, the data is automatically sorted by matching percentage and appended to your designated Google Sheet, complete with timestamps for clear, actionable reporting.

âš™ï¸ Fully Automated Workflow:

From fetching configuration to scraping, filtering, sorting, and uploading, the entire pipeline is 100% automated. Simply set it up once and let it run. It even prevents your computer from sleeping during operation!

ğŸ”§ How It Works: A Step-by-Step Flow

Configuration Load: The scraper starts by reading all its settings from your master Google Sheet.

Job Listing Scan: It navigates to the Indeed search URLs you provided, systematically going through each page of results.

AI Filtering (Batch Process): It collects a batch of job titles, sends them to the AI along with your resume, and gets a matching score for each job.

Detailed Scraping: For jobs that meet your minimum matching percentage, it visits the individual job page to extract full details: company, salary, full description, benefits, and more.

Classification: It determines if the job is an "Easy Apply," a "Company Site Apply," or from a "Confirmation Company" and categorizes it accordingly.

Local Save: The categorized data is appended to the correct local CSV file (Easy_applies.csv, etc.).

Final Upload: After all URLs are processed, the local CSV files are read, sorted by the highest match percentage, and beautifully appended to the corresponding tabs in your Google Sheet.

ğŸ› ï¸ Setup & Configuration Guide

Get your automated job lead engine running in just a few steps.

1. Prerequisites

Python 3.8+

Access to a Google Cloud Platform account.

A 2Captcha account with API key.

2. Installation

Clone the repository and install the required dependencies:

code
Bash
download
content_copy
expand_less

git clone <your-repository-url>
cd <repository-name>
pip install -r requirements.txt
3. Environment Variables

Create a .env file in the root directory and populate it with your credentials. This keeps your secrets safe.

# 2Captcha API Key for solving CAPTCHAs
2CAPTCHA_API_KEY="YOUR_2CAPTCHA_API_KEY"

# (Optional) For sending debugging screenshots via email
EMAIL_SENDER="your_email@gmail.com"
EMAIL_PASSWORD="your_app_password" # Use an App Password for Gmail
EMAIL_RECIPIENT="recipient_email@example.com"
SMTP_SERVER="smtp.gmail.com"
SMTP_PORT=587
4. Google Sheets & API Setup (The Control Center)

This is the most crucial step.

Create Your Google Sheet:

Create a new Google Sheet. The ID in the URL is your WORKBOOK_ID.

https://docs.google.com/spreadsheets/d/1Fbq9XRtBApCJHvjcrUI2JCIEGZC-Mri7-pt8hfHrSWI/edit

Create the following tabs (names must be exact):

Settings: For key-value configuration.

JobUrls: A single column with the Indeed search URLs to scrape.

IgnoreCompanies: A single column of company names to skip.

ConfirmationCompanies: A single column of high-priority company names.

Populate the Settings Tab:

Add key-value pairs like this (Column A is the key, Column B is the value):

Key	Value
WORKBOOK_ID	1Fbq9XRtBApCJHvjcrUI2JCIEGZC-Mri7-pt8hfHrSWI
MATCHING_PERCENTAGE	75
PER_COMPANY_JOBS	3
PROCESS_BATCH_SIZE	15
SHEETS_NAMES	Easy_applies,Confirmation_applies
LEAVE_BLANKS_COLLS	2
AI_PROMPT	Your detailed instructions for the AI...
RESUME	Your full resume text...

Set up Google Service Account:

Go to the Google Cloud Console.

Create a new project.

Enable the Google Drive API and Google Sheets API.

Go to Credentials -> Create Credentials -> Service Account.

Give it a name (e.g., "Sheets Scraper Bot"), grant it the Editor role.

After creating, click on the service account, go to the Keys tab, and Add Key -> Create new key -> JSON.

A JSON file will be downloaded. Rename it to gs_credentials.json and place it inside the utils/ directory.

Share Your Sheet:

Open gs_credentials.json and find the client_email value (e.g., python-api@...gserviceaccount.com).

In your Google Sheet, click Share and paste this email, giving it Editor access. This is essential for the script to work.

5. Input Data

Accounts: Place your exported Indeed cookies (in JSON format) inside the utils/accounts/ directory.

Fingerprints: Add browser fingerprint JSON files to the utils/fingerprints/ directory.

Proxies: Add your IP:PORT:USER:PASS formatted proxies to the proxies.txt file.

6. Running the Scraper

Once everything is configured, launch the scraper with a simple command:

python main.py

Sit back and watch as the console fills with progress updates. When it's done, your CSV files will be in the output/ folder and your Google Sheet will be populated with fresh, high-quality job leads.

ğŸ“ Project Structure
.
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config_input.py         # Loads all configuration from Google Sheets
â”œâ”€â”€ output/                     # Generated CSV files are stored here
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ job_details_scraper.py  # Scrapes individual job pages
â”‚   â””â”€â”€ job_listings_scraper.py # Scrapes the search result pages
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ accounts/               # Stores Indeed account cookies (JSON files)
â”‚   â”œâ”€â”€ bypass/                 # Logic for bypassing bot detection
â”‚   â”‚   â””â”€â”€ cloudflare.py       # Handles Cloudflare Turnstile CAPTCHA
â”‚   â”œâ”€â”€ fingerprints/           # Stores browser fingerprint profiles
â”‚   â”œâ”€â”€ accounts_loader.py      # Loads account cookies
â”‚   â”œâ”€â”€ fingerprint_loader.py   # Loads fingerprints
â”‚   â”œâ”€â”€ helper.py               # Core utilities (AI calls, file handling)
â”‚   â”œâ”€â”€ proxies_loader.py       # Loads proxies from file
â”‚   â””â”€â”€ sheet_uploader.py       # Manages CSV writing and Google Sheets upload
â”œâ”€â”€ .env                        # Your secret API keys and credentials
â”œâ”€â”€ main.py                     # The main entry point to run the application
â”œâ”€â”€ proxies.txt                 # List of proxies
â””â”€â”€ requirements.txt            # Project dependencies